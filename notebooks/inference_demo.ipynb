{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad3aa010",
   "metadata": {},
   "source": [
    "# The Shakespearean Scholar - Inference Notebook\n",
    "\n",
    "This notebook demonstrates the RAG system's capabilities for answering questions about Julius Caesar.\n",
    "\n",
    "**Make sure the Docker containers are running:**\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b09103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "BACKEND_URL = \"http://localhost:8000\"\n",
    "\n",
    "def query_rag(question: str, top_k: int = 5):\n",
    "    \"\"\"Query the RAG system\"\"\"\n",
    "    response = requests.post(\n",
    "        f\"{BACKEND_URL}/query\",\n",
    "        json={\"query\": question, \"top_k\": top_k, \"include_sources\": True}\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "def display_result(question: str, result: dict):\n",
    "    \"\"\"Display query result nicely\"\"\"\n",
    "    display(Markdown(f\"## Question\\n{question}\"))\n",
    "    display(Markdown(f\"## Answer\\n{result['answer']}\"))\n",
    "    display(Markdown(f\"**Confidence:** {result['confidence']:.2f}\"))\n",
    "    \n",
    "    display(Markdown(\"## Sources\"))\n",
    "    for i, source in enumerate(result['sources'], 1):\n",
    "        meta = source['metadata']\n",
    "        display(Markdown(\n",
    "            f\"**Source {i}:** Act {meta['act']}, Scene {meta['scene']} - {meta['speaker']}\\n\\n\"\n",
    "            f\"> {source['chunk'][:200]}...\\n\"\n",
    "        ))\n",
    "\n",
    "print(\"âœ… Utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf62f1",
   "metadata": {},
   "source": [
    "## Check System Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97721f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if backend is running\n",
    "response = requests.get(f\"{BACKEND_URL}/health\")\n",
    "health = response.json()\n",
    "print(f\"Status: {health['status']}\")\n",
    "print(f\"Vector Store Count: {health['vector_store_count']} chunks\")\n",
    "\n",
    "# Get stats\n",
    "stats = requests.get(f\"{BACKEND_URL}/stats\").json()\n",
    "print(f\"\\nEmbedding Model: {stats['embedding_model']}\")\n",
    "print(f\"Collection: {stats['collection_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516030a6",
   "metadata": {},
   "source": [
    "## Task 1: Factual Questions\n",
    "\n",
    "Testing the system's ability to retrieve and answer direct factual questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f0987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: Famous warning\n",
    "question = \"What does the Soothsayer say to Caesar?\"\n",
    "result = query_rag(question)\n",
    "display_result(question, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8bbf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: Caesar's death\n",
    "question = \"What are Caesar's last words?\"\n",
    "result = query_rag(question)\n",
    "display_result(question, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60093aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3: Final tribute\n",
    "question = \"What does Antony call Brutus at the end?\"\n",
    "result = query_rag(question)\n",
    "display_result(question, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d23582",
   "metadata": {},
   "source": [
    "## Task 2: Analytical Questions\n",
    "\n",
    "Testing the system's ability to handle complex analytical questions requiring synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe252809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical Question 1: Character analysis\n",
    "question = \"What are Brutus's internal conflicts as shown in his soliloquy in Act 2, Scene 1?\"\n",
    "result = query_rag(question, top_k=5)\n",
    "display_result(question, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808cd138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical Question 2: Rhetorical analysis\n",
    "question = \"What rhetorical devices does Antony use in his funeral oration?\"\n",
    "result = query_rag(question, top_k=5)\n",
    "display_result(question, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc53839",
   "metadata": {},
   "source": [
    "## Task 3: Comparative Questions\n",
    "\n",
    "Testing the system's ability to compare and contrast elements from different parts of the play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d69cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative Question\n",
    "question = \"Compare and contrast Brutus and Antony's speeches to the plebeians after Caesar's assassination.\"\n",
    "result = query_rag(question, top_k=7)\n",
    "display_result(question, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3219fb",
   "metadata": {},
   "source": [
    "## Task 4: Thematic Questions\n",
    "\n",
    "Testing understanding of broader themes in the play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thematic Question\n",
    "question = \"What role do omens and supernatural elements play in the tragedy?\"\n",
    "result = query_rag(question, top_k=5)\n",
    "display_result(question, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c359651a",
   "metadata": {},
   "source": [
    "## Task 5: Batch Evaluation\n",
    "\n",
    "Process multiple questions and analyze performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch query test\n",
    "test_questions = [\n",
    "    \"Who is Octavius?\",\n",
    "    \"Why do Brutus and Cassius argue?\",\n",
    "    \"What appears at Brutus's bedside in camp?\",\n",
    "    \"How does Cassius die?\",\n",
    "    \"How does Brutus die?\"\n",
    "]\n",
    "\n",
    "batch_response = requests.post(\n",
    "    f\"{BACKEND_URL}/batch_query\",\n",
    "    json={\"queries\": test_questions, \"top_k\": 3}\n",
    ")\n",
    "batch_results = batch_response.json()\n",
    "\n",
    "print(f\"Processed {batch_results['total']} questions\\n\")\n",
    "\n",
    "# Show summary\n",
    "for result in batch_results['results']:\n",
    "    print(f\"Q: {result['question']}\")\n",
    "    print(f\"A: {result['answer'][:150]}...\")\n",
    "    print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82663536",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51452ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average confidence\n",
    "confidences = [r['confidence'] for r in batch_results['results']]\n",
    "avg_confidence = sum(confidences) / len(confidences)\n",
    "\n",
    "print(f\"Average Confidence: {avg_confidence:.3f}\")\n",
    "print(f\"Min Confidence: {min(confidences):.3f}\")\n",
    "print(f\"Max Confidence: {max(confidences):.3f}\")\n",
    "\n",
    "# Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(confidences)), confidences)\n",
    "plt.axhline(y=avg_confidence, color='r', linestyle='--', label='Average')\n",
    "plt.xlabel('Question Number')\n",
    "plt.ylabel('Confidence Score')\n",
    "plt.title('RAG System Confidence Scores')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db96b544",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the RAG system's capabilities across different question types:\n",
    "\n",
    "1. **Factual Questions**: Direct retrieval from the text\n",
    "2. **Analytical Questions**: Synthesis and interpretation\n",
    "3. **Comparative Questions**: Cross-referencing multiple parts\n",
    "4. **Thematic Questions**: Understanding broader patterns\n",
    "\n",
    "The system successfully provides accurate, cited answers appropriate for ICSE Class 10 students."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
